# ECG Heartbeat Arrhythmia Classification

A PyTorch-based exploration and comparison of sequence models (Vanilla RNN, LSTM, GRU) for ECG heartbeat classification using the MIT-BIH Arrhythmia dataset (Kaggle "heartbeat" dataset). This project trains and evaluates models on the 187-timestep heartbeat signals and includes preprocessing, imbalance handling, training, evaluation, visualizations, and artifacts to reproduce results.

---

## Highlights

- Dataset: MIT-BIH Arrhythmia (Kaggle `shayanfazeli/heartbeat`) — `mitbih_train.csv` and `mitbih_test.csv` used.
- Models implemented in PyTorch: Vanilla RNN, LSTM, GRU.
- Techniques used: standardization (train stats), stratified train/val split, WeightedRandomSampler for imbalanced classes, gradient clipping, dropout, ReduceLROnPlateau scheduler, early stopping.
- Device: macOS MPS support is used when available (notebook auto-detects `mps` or falls back to `cpu`).
- Outputs saved: best model weights (/*.pth), `project_results.pkl`, `model_summary.csv`, `per_class_f1.csv`.

---

## Quick results (from notebook run)

> These numbers are pulled from the notebook run embedded in `code.ipynb`.

- GRU: Test Accuracy = 97.73%, Macro F1 ≈ (see per-class table)
- LSTM: Test Accuracy = 96.76%
- Vanilla RNN: Test Accuracy = 39.93% (demonstrates vanishing gradient issues on long sequences)

See the `model_summary.csv` and `per_class_f1.csv` files saved by the notebook for full numeric tables and per-class metrics.

---

## Repository structure

- `code.ipynb` — Main notebook with the full pipeline: data loading, preprocessing, model definitions, training loops, evaluation, visualizations, and saving artifacts.
- `requirements.txt` — Minimal dependencies used for the notebook.
- `README_venv_setup.md` — Venv activation and installation instructions (created alongside this README).
- `.gitignore` — Project gitignore (ignores `venv/`, `heartbeat.zip`, model checkpoints, etc.).
- `heartbeat.zip` — (ignored) Suggested dataset archive (do not commit large datasets to repo).
- Artifacts generated by notebook (after running): `best_lstm.pth`, `best_gru.pth`, `best_vanilla_rnn.pth`, `project_results.pkl`, `model_summary.csv`, `per_class_f1.csv`.

---

## How to reproduce (recommended)

1. Clone this repository.

2. Create and activate a Python virtual environment (example):

```bash
python -m venv venv
source venv/bin/activate
```

3. Install dependencies:

```bash
python -m pip install --upgrade pip setuptools wheel
python -m pip install -r requirements.txt
# For macOS Apple Silicon MPS-optimized PyTorch, prefer the exact command from https://pytorch.org/get-started/locally/
# Example that often works:
python -m pip install torch torchvision torchaudio
```

4. Download the dataset (Kaggle):

- Option A (via Kaggle CLI):

```bash
python -m pip install kaggle
# configure your Kaggle API token (KAGGLE_TOKEN) or ~/.kaggle/kaggle.json
kaggle datasets download -d shayanfazeli/heartbeat
unzip heartbeat.zip
```

This yields `mitbih_train.csv` and `mitbih_test.csv` in the repo root. Alternatively, move the dataset CSVs to the project root and update the `TRAIN_PATH`/`TEST_PATH` variables in `code.ipynb`.

5. Register the environment as a Jupyter kernel (optional but recommended for VS Code notebooks):

```bash
python -m ipykernel install --user --name=ecg_venv --display-name "Python (ecg_venv)"
```

Then select "Python (ecg_venv)" in VS Code notebook kernel selector.

6. Open `code.ipynb` and run cells in order. The notebook covers:
- Data loading and exploration
- Normalization (standardization using training statistics)
- Stratified train/validation split
- `ECGDataset` PyTorch Dataset
- WeightedRandomSampler to address class imbalance
- DataLoader creation
- Model definitions (Vanilla RNN, LSTM, GRU)
- Training loops with LR scheduler, gradient clipping, early stopping
- Evaluation, confusion matrices, ROC/PR curves, learning curves
- Saving results as `.pth`, `.pkl`, and `.csv` files

Note: training the models can take significant time depending on hardware. The notebook includes hyperparameters and training-time logging; you can reduce `NUM_EPOCHS` or batch size for quicker runs while experimenting.

---

## Important hyperparameters (defaults in notebook)

- Batch size: 64
- Learning rate: 0.001
- Optimizer: Adam
- Loss: CrossEntropyLoss
- Num epochs: 50 (with early stopping patience of 10)
- Gradient clipping: 1.0
- WeightedRandomSampler used to mitigate class imbalance

---

## Artifacts produced by the notebook

- `best_vanilla_rnn.pth`, `best_lstm.pth`, `best_gru.pth` — saved best model weights
- `project_results.pkl` — pickled dictionary containing `test_results`, `history`, `training_times`, `model_info`, and `class_names`
- `model_summary.csv` — summary table (model params, accuracy, F1 etc.)
- `per_class_f1.csv` — per-class F1-scores for each model

You can use these artifacts to reproduce the figures in the notebook or to build a shorter inference script.

---

## Notes & tips

- The notebook demonstrates why gating (LSTM/GRU) is crucial for long ECG sequences: Vanilla RNN struggled due to vanishing gradients.
- The project uses weighted sampling + regularization (dropout, gradient clipping) and LR scheduling to achieve stable training and good minority-class performance.
- On macOS Apple Silicon (M1/M2/M3/M4), enable MPS by ensuring you have a compatible PyTorch build; the notebook auto-detects and uses `mps` when available.
- `heartbeat.zip` is intentionally added to `.gitignore` to avoid pushing large data files. Keep the raw dataset offline or in an external storage provider and use Kaggle CLI to fetch as needed.

---

## Next steps / possible improvements

- Convert training cells to standalone scripts (e.g., `train.py`) with CLI flags for quick experiments and CI.
- Add model checkpointing with best-validation-metric selection and resume-from-checkpoint functionality.
- Add unit tests for data loading and model forward shapes.
- Experiment with 1D CNNs or hybrid CNN-RNNs for faster training and comparable accuracy.
- Add Dockerfile or GitHub Actions workflow for reproducible runs.

---

## License

This repository is provided under the MIT License — change as needed.

---

## Contact

If you use or extend this project, please open an issue or PR. You can also email the author: (replace with your email/contact).
